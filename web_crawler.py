import requests, os, sys, bs4
from urllib.parse import urljoin


domain = sys.argv[1]

content_list = []

with open(f'recon/{domain}/crawler_output','w') as file:
    pass

session = requests.Session()

def request(url):
    try:
        html = session.get(url, allow_redirects=False, timeout=2, cookies={
            "id":"89649ef1-17e0-4153-b625-324d6de362c9","org.springframework.web.servlet.i18n.CookieLocaleResolver.LOCALE":"en","_gcl_au":"1.1.92754793.1697624002","c_login_token":"1697042303427","_ga_KTJBTB7K4C":"GS1.2.1697646625.1.1.1697646667.0.0.0","_ga":"GA1.1.566411941.1697624002","amp_e56929":"fqbjrtiAJiFJl-61nvtG_N...1hdb2r8su.1hdb2r8su.0.0.0","amp_e56929_virtualcyberlabs.com":"fqbjrtiAJiFJl-61nvtG_N.NjQwMzExMzZlNGIwYjMxMjRmZDRiZDEz..1hdb2r8u1.1hdb2r90j.j.l.18", "_ga_QBNBN7VB0P":"GS1.1.1698005584.6.0.1698005584.0.0.0","_ga_LCX7T69G50":"GS1.1.1698358429.1.0.1698358429.0.0.0", "_ga_Z6RLW1K3NV":"GS1.1.1698358427.1.0.1698358632.0.0.0","SESSIONID":"5CCD33901777DE759D91CB67793D8EDB","crisp-client%2Fsession%2F63b16071-fed9-4938-b4b3-51c7f9a3b547":"session_5df2b0b8-d1b2-4948-b796-032ff515fcfd","_ga_RFN4WR1QXJ":"GS1.1.1698358644.6.1.1698358648.0.0.0","_ga_XQP1LYD7B4":"GS1.1.1698358645.5.1.1698358649.0.0.0"})
        return html.content
    except Exception as e:
        print(e)
        return ''

def crawl(url):

    try:
        html = request(url)
        soup = bs4.BeautifulSoup(html,'html.parser')
        for a in soup.find_all('a', href=True):
            link = urljoin(url,a['href'])

            if '#' in link:
                link = link.split('#')[0]

            if link not in content_list and domain in link and "cloudfront" :
                content_list.append(link)
                print("[+] Found the URL: {}".format(link))
                with open(f'recon/{domain}/crawler_output','a') as file:
                    file.write(link + '\n')
                crawl(link)
    except KeyboardInterrupt:
        exit(0)


with open(f'recon/{domain}/subdomains','r') as file:
    subdomains = file.read().splitlines()
    for subdomain in subdomains:
        url = f"http://{subdomain}"
        crawl(url)